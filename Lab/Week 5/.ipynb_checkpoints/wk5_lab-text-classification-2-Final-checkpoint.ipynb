{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Clasification II (week 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab is prepared with the materials in the article \"A Comprehensive Guide to Understand and Implement Text Classification in Python\" https://www.analyticsvidhya.com/blog/2018/04/a-comprehensive-guide-to-understand-and-implement-text-classification-in-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load libraries for dataset preparation, feature engineering, model training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "\n",
    "# install xgboost and textblob: $>pip install xgboost; $>pip install textblob\n",
    "import pandas, xgboost, numpy, textblob, string  \n",
    "\n",
    "# load functions from textpreprocess.py\n",
    "from textpreprocess import denoise_text, normalize, replace_contractions, remove_non_ascii, to_lowercase, remove_punctuation, replace_numbers, remove_stopwords\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Dataset preparation\n",
    "We are using the dataset of amazon reviews which can be downloaded at this link https://gist.github.com/kunalj101/ad1d9c58d338e20d09ff26bcc06c4235 The dataset consists of 10,000 text reviews and their labels, To prepare the dataset, load the downloaded data into a pandas dataframe containing two columns – text and label.\n",
    "\n",
    "<b>We are taking only 1,000 reviews for each category.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset, but load only 1000 reviews for each category.\n",
    "data = open('data/corpus', encoding=\"utf-8\").read()\n",
    "labels, texts = [], [] # dictionary for labels and texts\n",
    "label_1_count = 0\n",
    "label_2_count = 0\n",
    "for i, line in enumerate(data.split(\"\\n\")):\n",
    "    # load 1000 reviews for each category\n",
    "    if label_1_count < 1000 or label_2_count < 1000:\n",
    "        line = replace_contractions(line) # Replace contractions in string of text\n",
    "        content = nltk.word_tokenize(line)   \n",
    "        words = content[1:] # first token is the label\n",
    "        words = remove_non_ascii(words)\n",
    "        #words = to_lowercase(words)\n",
    "        words = remove_punctuation(words)\n",
    "        #words = replace_numbers(words)\n",
    "        #words = remove_stopwords(words)\n",
    "        \n",
    "        if content[0] == '__label__1' and label_1_count < 1000:\n",
    "            label_1_count += 1 # add count to label 1\n",
    "            labels.append(content[0])\n",
    "            texts.append(words)\n",
    "        if content[0] == '__label__2' and label_2_count < 1000:\n",
    "            label_2_count += 1 # add count to label 2\n",
    "            labels.append(content[0])\n",
    "            texts.append(words)\n",
    "        \n",
    "# create a dataframe using texts and labels\n",
    "trainDF = pandas.DataFrame()\n",
    "texts1=[' '.join(line) for line in texts] # join words in each line with space character\n",
    "trainDF['text'] = texts1\n",
    "trainDF['label'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF.shape #2000 rows, 2 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__label__2    1000\n",
       "__label__1    1000\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF['label'].value_counts(sort=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will split the dataset into training and validation sets so that we can train and test classifier. Also, we will encode our target column so that it can be used in machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_y: \n",
      "__label__2    750\n",
      "__label__1    750\n",
      "Name: label, dtype: int64\n",
      "\n",
      "Valid_y: \n",
      "__label__2    250\n",
      "__label__1    250\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# split the dataset into training and validation datasets: 75% for training, 25% (default) for testing\n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(trainDF['text'], \n",
    "                                           trainDF['label'],random_state=2, stratify=trainDF['label'])\n",
    "\n",
    "print(\"Train_y: \")\n",
    "print(train_y.value_counts())\n",
    "print()\n",
    "print(\"Valid_y: \")\n",
    "print(valid_y.value_counts())\n",
    "# label encode the target variable \n",
    "# => holds the labels in an array\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.transform(valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__label__1', '__label__2']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.transform(['__label__1', '__label__2']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__label__1', '__label__2', '__label__1', '__label__2']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(encoder.inverse_transform([0, 1, 0, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Feature Engineering\n",
    "The next step is the feature engineering step. In this step, raw text data will be transformed into feature vectors and new features will be created using the existing dataset. We will implement the following different ideas in order to obtain relevant features from our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Count Vectors as features\n",
    "[Count Vector](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) is a matrix notation of the dataset in which every row represents a document from the corpus, every column represents a term from the corpus, and every cell represents the frequency count of a particular term in a particular document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a count vectorizer object: \n",
    "# analyzer: whether the feature should be made of word or character n-grams.\n",
    "# token_pattern: regular expression denoting what constitutes a “token”\n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "\n",
    "# fit and transform the training and validation data using count vectorizer object\n",
    "xtrain_count =  count_vect.fit_transform(train_x)\n",
    "xvalid_count =  count_vect.transform(valid_x) # just transform validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 TF-IDF Vectors as features\n",
    "\n",
    "[TF-IDF Vectors](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) can be generated at different levels of input tokens (words, characters, n-grams)\n",
    "\n",
    "a. Word Level TF-IDF : Matrix representing tf-idf scores of every term in different documents\n",
    "\n",
    "b. N-gram Level TF-IDF : N-grams are the combination of N terms together. This Matrix representing tf-idf scores of N-grams\n",
    "\n",
    "c. Character Level TF-IDF : Matrix representing tf-idf scores of character level n-grams in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "xtrain_tfidf =  tfidf_vect.fit_transform(train_x)\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
    "\n",
    "# ngram level tf-idf \n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "xtrain_tfidf_ngram =  tfidf_vect_ngram.fit_transform(train_x)\n",
    "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n",
    "\n",
    "# characters level tf-idf\n",
    "# Regular expression denoting what constitutes a “token”, only used if analyzer == 'word'. \n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.fit_transform(train_x) \n",
    "xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model Building\n",
    "The next step in the text classification framework is to train a classifier using the features created in the previous step. There are many different choices of machine learning models which can be used to train a final model. We will implement Naive Bayes Classifier for this purpose:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is a utility function which can be used to train a model. It accepts the classifier, feature_vector of training data, labels of training data and feature vectors of valid data as inputs. Using these inputs, the model is trained and accuracy score is computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    \n",
    "    return metrics.accuracy_score(predictions, valid_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Naive Bayes Classifier\n",
    "Implementing a naive bayes model using sklearn implementation with different features\n",
    "\n",
    "[Naive Bayes](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html) is a classification technique based on Bayes’ Theorem with an assumption of independence among predictors. A Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature here ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, Count Vectors:  0.82\n",
      "NB, WordLevel TF-IDF:  0.822\n",
      "NB, N-Gram Vectors:  0.828\n",
      "NB, CharLevel Vectors:  0.794\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes on Count Vectors\n",
    "# train_model(classifier, train feature vectors (countvectorizer, label, \n",
    "# validation feature vectors(countvectorizer))\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count)\n",
    "print (\"NB, Count Vectors: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Word Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print (\"NB, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Ngram Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print (\"NB, N-Gram Vectors: \", accuracy)\n",
    "\n",
    "# Naive Bayes on Character Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
    "print (\"NB, CharLevel Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Logistic Regression\n",
    "\n",
    "Implementing a Linear Classifier (Logistic Regression)\n",
    "\n",
    "[Logistic regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) measures the relationship between the categorical dependent variable and one or more independent variables by estimating probabilities using a logistic/sigmoid function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, Count Vectors:  0.838\n",
      "LR, WordLevel TF-IDF:  0.866\n",
      "LR, N-Gram Vectors:  0.824\n",
      "LR, CharLevel Vectors:  0.842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robin/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Linear Classifier on Count Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_count, train_y, xvalid_count)\n",
    "print (\"LR, Count Vectors: \", accuracy)\n",
    "\n",
    "# Linear Classifier on Word Level TF IDF Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print (\"LR, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "# Linear Classifier on Ngram Level TF IDF Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print (\"LR, N-Gram Vectors: \", accuracy)\n",
    "\n",
    "# Linear Classifier on Character Level TF IDF Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
    "print (\"LR, CharLevel Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 SVM Classifer\n",
    "\n",
    "[Support Vector Machine (SVM)](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) is a supervised machine learning algorithm which can be used for both classification or regression challenges. The model extracts a best possible hyper-plane / line that segregates the two classes. \n",
    "\n",
    "The implementation of SVM is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to dataset with more than a couple of 10000 samples.\n",
    "The multiclass support is handled according to a one-vs-one scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM, Count Vectors:  0.814\n",
      "SVM, WordLevel TF-IDF:  0.866\n",
      "SVM, N-Gram Vectors:  0.802\n",
      "SVM, CharLevel Vectors:  0.836\n"
     ]
    }
   ],
   "source": [
    "# SVM Classifier on Count Vectors\n",
    "accuracy = train_model(svm.SVC(kernel='linear'), xtrain_count, train_y, xvalid_count)\n",
    "print (\"SVM, Count Vectors: \", accuracy)\n",
    "\n",
    "# SVM Classifier on Word Level TF IDF Vectors\n",
    "accuracy = train_model(svm.SVC(kernel='linear'), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print (\"SVM, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "# SVM Classifier on Ngram Level TF IDF Vectors\n",
    "accuracy = train_model(svm.SVC(kernel='linear'), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print (\"SVM, N-Gram Vectors: \", accuracy)\n",
    "\n",
    "# SVM Classifier on Character Level TF IDF Vectors\n",
    "accuracy = train_model(svm.SVC(kernel='linear'), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
    "print (\"SVM, CharLevel Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 Radom Forest Classifier\n",
    "\n",
    "Implementing a [Random Forest Model](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
    "\n",
    "Random Forest models are a type of ensemble models, particularly bagging models. They are part of the tree based model family."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF, Count Vectors:  0.808\n",
      "RF, WordLevel TF-IDF:  0.822\n",
      "RF, N-Gram Vectors:  0.736\n",
      "RF, CharLevel Vectors:  0.778\n"
     ]
    }
   ],
   "source": [
    "# RF on Count Vectors => n_estimators = number of trees in forest\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(n_estimators=100), xtrain_count, train_y, xvalid_count)\n",
    "print (\"RF, Count Vectors: \", accuracy)\n",
    "\n",
    "# RF on Word Level TF IDF Vectors\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(n_estimators=100), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print (\"RF, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "# RF Classifier on Ngram Level TF IDF Vectors\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(n_estimators=100), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print (\"RF, N-Gram Vectors: \", accuracy)\n",
    "\n",
    "# RF Classifier on Character Level TF IDF Vectors\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(n_estimators=100), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
    "print (\"RF, CharLevel Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5 Boosting Model\n",
    "\n",
    "Implementing [Xtereme Gradient Boosting Model](https://xgboost.readthedocs.io/en/latest/index.html).\n",
    "\n",
    "Boosting models are another type of ensemble models part of tree based models. Boosting is a machine learning ensemble meta-algorithm for primarily reducing bias, and also variance in supervised learning, and a family of machine learning algorithms that convert weak learners to strong ones. A weak learner is defined to be a classifier that is only slightly correlated with the true classification (it can label examples better than random guessing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xgb, Count Vectors:  0.808\n",
      "Xgb, WordLevel TF-IDF:  0.814\n",
      "Xgb, N-Gram Vectors:  0.712\n",
      "Xgb, CharLevel Vectors:  0.77\n"
     ]
    }
   ],
   "source": [
    "# Extereme Gradient Boosting on Count Vectors\n",
    "accuracy = train_model(xgboost.XGBClassifier(), xtrain_count, train_y, xvalid_count)\n",
    "print (\"Xgb, Count Vectors: \", accuracy)\n",
    "\n",
    "# Extereme Gradient Boosting on Word Level TF IDF Vectors\n",
    "accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print (\"Xgb, WordLevel TF-IDF: \", accuracy)\n",
    "\n",
    "# RF Classifier on Ngram Level TF IDF Vectors\n",
    "accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print (\"Xgb, N-Gram Vectors: \", accuracy)\n",
    "\n",
    "# Extereme Gradient Boosting on Character Level TF IDF Vectors\n",
    "accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
    "print (\"Xgb, CharLevel Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Grid Search with SVM - improve performance through grid search of parameters\n",
    "\n",
    "[GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) performs exhaustive search over specified parameter values for an estimator.\n",
    "Important members are fit, predict.\n",
    "GridSearchCV implements a “fit” and a “score” method. It also implements “predict”, “predict_proba”, “decision_function”, “transform” and “inverse_transform” if they are implemented in the estimator used.\n",
    "The parameters of the estimator used to apply these methods are optimized by cross-validated grid-search over a parameter grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    6.1s\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:    9.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.803\n",
      "Best parameters set:\n",
      "\tclf__C: 1\n",
      "\tclf__kernel: 'linear'\n",
      "Accuracy: 0.866\n",
      "Precision: 0.8765432098765432\n",
      "Recall: 0.852\n",
      "F1_score: 0.8640973630831643\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)),\n",
    "    ('clf', svm.SVC())\n",
    "])\n",
    "parameters = {\n",
    "    #'vect__max_df': (0.1, 0.5, 1.0),\n",
    "    #'vect__stop_words': ('english', None),\n",
    "    #'vect__lowercase': (True, False),\n",
    "    #'vect__binary': (True, False),\n",
    "    #'vect__max_features': (5000, 7500),\n",
    "    #'vect__ngram_range': ((1, 1), (1, 2)),\n",
    "    #'vect__use_idf': (True, False),\n",
    "    #'vect__norm': ('l1', 'l2'),\n",
    "    'clf__C': (0.01, 1, 10),\n",
    "    #'clf__gamma: (0.5, 1,2,3,4),\n",
    "    'clf__kernel': ('rbf', 'linear')\n",
    "}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1, scoring='accuracy', cv=10)\n",
    "    grid_search.fit(train_x, train_y)\n",
    "    print('Best score: %0.3f' % grid_search.best_score_)\n",
    "    print('Best parameters set:')\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print('\\t%s: %r' % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "    # Refit an estimator using the best found parameters on the whole dataset.\n",
    "    # The refitted estimator is made available at the best_estimator_attribute and \n",
    "    # permits using predict directly on this GridSearchCV instance.\n",
    "    predictions = grid_search.predict(valid_x)\n",
    "    print('Accuracy:', accuracy_score(valid_y, predictions))\n",
    "    print('Precision:', precision_score(valid_y, predictions))\n",
    "    print('Recall:', recall_score(valid_y, predictions))\n",
    "    print('F1_score:', f1_score(valid_y, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to do Grid Search without pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.766      0.766      0.766      0.77333333 0.766      0.80666667\n",
      " 0.766      0.792      0.766      0.792     ]\n",
      "[{'C': 0.01, 'kernel': 'rbf'}, {'C': 0.01, 'kernel': 'linear'}, {'C': 0.1, 'kernel': 'rbf'}, {'C': 0.1, 'kernel': 'linear'}, {'C': 1, 'kernel': 'rbf'}, {'C': 1, 'kernel': 'linear'}, {'C': 10, 'kernel': 'rbf'}, {'C': 10, 'kernel': 'linear'}, {'C': 100, 'kernel': 'rbf'}, {'C': 100, 'kernel': 'linear'}]\n"
     ]
    }
   ],
   "source": [
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'C': [0.01, 0.1, 1, 10, 100],\n",
    "                     'kernel': ['rbf', 'linear']}]\n",
    "clf = GridSearchCV(svm.SVC(), tuned_parameters, cv=10, scoring='accuracy')\n",
    "clf.fit(xtrain_tfidf, train_y)\n",
    "#clf.grid_scores_\n",
    "#clf.cv_results_\n",
    "print(clf.cv_results_['mean_test_score'])\n",
    "print(clf.cv_results_['params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1, 'kernel': 'linear'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.866"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 15% of test observations misclassified\n",
    "clf.best_estimator_.score(xvalid_tfidf, valid_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Combining text with other features using pipeline: Text / NLP based features\n",
    "\n",
    "With pipelines, you don't need to carry test dataset transformation along with your train features - this is taken care of automatically.\n",
    "\n",
    "Refer to [A Deep Dive Into Sklearn Pipelines](https://www.kaggle.com/baghern/a-deep-dive-into-sklearn-pipelines/data) and [Work like a Pro with Pipelines and Feature Unions](https://www.kaggle.com/metadist/work-like-a-pro-with-pipelines-and-feature-unions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF['char_count'] = trainDF['text'].apply(len)\n",
    "#lambda creates the inline function => x: len(x.split())\n",
    "trainDF['word_count'] = trainDF['text'].apply(lambda x: len(x.split())) \n",
    "trainDF['word_density'] = trainDF['char_count'] / (trainDF['word_count']+1)\n",
    "trainDF['punctuation_count'] = trainDF['text'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation))) \n",
    "trainDF['title_word_count'] = trainDF['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()]))\n",
    "trainDF['upper_case_word_count'] = trainDF['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>char_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>word_density</th>\n",
       "      <th>punctuation_count</th>\n",
       "      <th>title_word_count</th>\n",
       "      <th>upper_case_word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Stuning even for the nongamer This sound track...</td>\n",
       "      <td>__label__2</td>\n",
       "      <td>416</td>\n",
       "      <td>80</td>\n",
       "      <td>5.135802</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The best soundtrack ever to anything I am read...</td>\n",
       "      <td>__label__2</td>\n",
       "      <td>505</td>\n",
       "      <td>101</td>\n",
       "      <td>4.950980</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amazing This soundtrack is my favorite music o...</td>\n",
       "      <td>__label__2</td>\n",
       "      <td>732</td>\n",
       "      <td>134</td>\n",
       "      <td>5.422222</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Excellent Soundtrack I truly like this soundtr...</td>\n",
       "      <td>__label__2</td>\n",
       "      <td>711</td>\n",
       "      <td>118</td>\n",
       "      <td>5.974790</td>\n",
       "      <td>0</td>\n",
       "      <td>51</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Remember Pull Your Jaw Off The Floor After Hea...</td>\n",
       "      <td>__label__2</td>\n",
       "      <td>465</td>\n",
       "      <td>90</td>\n",
       "      <td>5.109890</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text       label  char_count  \\\n",
       "0  Stuning even for the nongamer This sound track...  __label__2         416   \n",
       "1  The best soundtrack ever to anything I am read...  __label__2         505   \n",
       "2  Amazing This soundtrack is my favorite music o...  __label__2         732   \n",
       "3  Excellent Soundtrack I truly like this soundtr...  __label__2         711   \n",
       "4  Remember Pull Your Jaw Off The Floor After Hea...  __label__2         465   \n",
       "\n",
       "   word_count  word_density  punctuation_count  title_word_count  \\\n",
       "0          80      5.135802                  1                10   \n",
       "1         101      4.950980                  0                11   \n",
       "2         134      5.422222                  0                25   \n",
       "3         118      5.974790                  0                51   \n",
       "4          90      5.109890                  0                31   \n",
       "\n",
       "   upper_case_word_count  \n",
       "0                      3  \n",
       "1                      6  \n",
       "2                      4  \n",
       "3                      4  \n",
       "4                      0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftrain_x, fvalid_x, train_y, valid_y = model_selection.train_test_split(trainDF[['text', 'char_count', 'word_count', 'word_density', 'punctuation_count', 'title_word_count', 'upper_case_word_count']], trainDF['label'], \n",
    "                                                                      random_state=2, stratify=trainDF['label'])\n",
    "# label encode the target variable \n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.transform(valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>char_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>word_density</th>\n",
       "      <th>punctuation_count</th>\n",
       "      <th>title_word_count</th>\n",
       "      <th>upper_case_word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1468</th>\n",
       "      <td>tobacco pouch The material and workmanship is ...</td>\n",
       "      <td>226</td>\n",
       "      <td>38</td>\n",
       "      <td>5.794872</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>Intelligently written a fast and suspenseful r...</td>\n",
       "      <td>440</td>\n",
       "      <td>81</td>\n",
       "      <td>5.365854</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1047</th>\n",
       "      <td>the best album by a femalelead group I love th...</td>\n",
       "      <td>253</td>\n",
       "      <td>53</td>\n",
       "      <td>4.685185</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>802</th>\n",
       "      <td>too had to read I find the old english wording...</td>\n",
       "      <td>163</td>\n",
       "      <td>36</td>\n",
       "      <td>4.405405</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1542</th>\n",
       "      <td>Bad run the FIRST time I wore them Having read...</td>\n",
       "      <td>321</td>\n",
       "      <td>59</td>\n",
       "      <td>5.350000</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  char_count  \\\n",
       "1468  tobacco pouch The material and workmanship is ...         226   \n",
       "459   Intelligently written a fast and suspenseful r...         440   \n",
       "1047  the best album by a femalelead group I love th...         253   \n",
       "802   too had to read I find the old english wording...         163   \n",
       "1542  Bad run the FIRST time I wore them Having read...         321   \n",
       "\n",
       "      word_count  word_density  punctuation_count  title_word_count  \\\n",
       "1468          38      5.794872                  0                 3   \n",
       "459           81      5.365854                  0                12   \n",
       "1047          53      4.685185                  0                 4   \n",
       "802           36      4.405405                  0                 1   \n",
       "1542          59      5.350000                  0                 8   \n",
       "\n",
       "      upper_case_word_count  \n",
       "1468                      1  \n",
       "459                       2  \n",
       "1047                      2  \n",
       "802                       1  \n",
       "1542                      5  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ftrain_x.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to tag our text in the pipeline, we will create an estimator class of our own. We just have to inherit some base classes and overload very few functions that we are actually going to use.\n",
    "\n",
    "First we are going to create a selector transformer that simply returns the one column in the dataset by the key value we pass. We made two different selectors for either text or numeric columns. The return type is different, but other than that they work the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class TextSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformer to select a single column from the data frame to perform additional transformations on\n",
    "    Use on text columns in the data\n",
    "    \"\"\"\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, X, y=None): # fit() doesn't do anything\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):   # all the work is done here\n",
    "        return X[self.key]\n",
    "    \n",
    "class NumberSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformer to select a single column from the data frame to perform additional transformations on\n",
    "    Use on numeric columns in the data\n",
    "    \"\"\"\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[[self.key]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline consists of two steps: first grab just text column from the dataset, then perform tf-idf on just that column and return the results.\n",
    "To make a pipeline, just pass an array of tuples of the format (name, object). The first part is the name of the action, and the second is the actual object. So this pipeline consists of \"selecting\" and then \"tfidf-ing\" a column.\n",
    "To execute, use it just like any other transformer. You can call text.fit() to fit to training data, text.transform() to apply it to training data, or text.fit_transform() to do both.\n",
    "Since it's text, it will return a sparse matrix, but we can see that it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1500x11468 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 46047 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "text = Pipeline([\n",
    "                ('selector', TextSelector(key='text')),\n",
    "                ('tfidf', TfidfVectorizer(stop_words='english'))\n",
    "            ])\n",
    "\n",
    "text.fit_transform(ftrain_x)  # create 1500x11480 matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('selector', TextSelector(key='text')),\n",
       " ('tfidf',\n",
       "  TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                  dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                  input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                  min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                  smooth_idf=True, stop_words='english', strip_accents=None,\n",
       "                  sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                  tokenizer=None, use_idf=True, vocabulary=None))]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.steps #  look up all the steps of the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.88800316],\n",
       "       [ 0.05642127],\n",
       "       [-0.7688468 ],\n",
       "       ...,\n",
       "       [-0.33635338],\n",
       "       [-1.04687829],\n",
       "       [-0.48640212]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "char_count =  Pipeline([\n",
    "                ('selector', NumberSelector(key='char_count')),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "\n",
    "char_count.fit_transform(ftrain_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count =  Pipeline([\n",
    "                ('selector', NumberSelector(key='word_count')),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "word_density =  Pipeline([\n",
    "                ('selector', NumberSelector(key='word_density')),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "punctuation_count =  Pipeline([\n",
    "                ('selector', NumberSelector(key='punctuation_count')),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "title_word_count =  Pipeline([\n",
    "                ('selector', NumberSelector(key='title_word_count')),\n",
    "                ('standard', StandardScaler()),\n",
    "            ])\n",
    "upper_case_word_count =  Pipeline([\n",
    "                ('selector', NumberSelector(key='upper_case_word_count')),\n",
    "                ('standard', StandardScaler()),\n",
    "            ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make a pipeline from all of our pipelines, we do the same thing, but now we use a FeatureUnion to join the feature processing pipelines. The syntax is the same as a regular pipeline, it's just an array of tuple, with the (name, object) format.\n",
    "\n",
    "The feature union itself is not a pipeline, it's just a union, so you need to do one more step to make it useable: pass it to a pipeline, with the same structure, an array of tuples, with the simple (name, object) format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1500x11474 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 55047 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "feats = FeatureUnion([('text', text), \n",
    "                      ('char_count', char_count),\n",
    "                      ('word_count', word_count),\n",
    "                      ('word_density', word_density),\n",
    "                      ('punctuation_count', punctuation_count),\n",
    "                      ('title_word_count', title_word_count),\n",
    "                      ('upper_case_word_count', upper_case_word_count)])\n",
    "\n",
    "feature_processing = Pipeline([('feats', feats)])\n",
    "feature_processing.fit_transform(ftrain_x)  # create 1500x11486 matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features',feats),\n",
    "    ('classifier', svm.SVC())\n",
    "])\n",
    "\n",
    "pipeline.fit(ftrain_x, train_y)\n",
    "\n",
    "preds = pipeline.predict(fvalid_x)\n",
    "np.mean(preds == valid_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the list of all the possible things you could fine tune, call get_params().keys() on your pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['memory', 'steps', 'verbose', 'features', 'classifier', 'features__n_jobs', 'features__transformer_list', 'features__transformer_weights', 'features__verbose', 'features__text', 'features__char_count', 'features__word_count', 'features__word_density', 'features__punctuation_count', 'features__title_word_count', 'features__upper_case_word_count', 'features__text__memory', 'features__text__steps', 'features__text__verbose', 'features__text__selector', 'features__text__tfidf', 'features__text__selector__key', 'features__text__tfidf__analyzer', 'features__text__tfidf__binary', 'features__text__tfidf__decode_error', 'features__text__tfidf__dtype', 'features__text__tfidf__encoding', 'features__text__tfidf__input', 'features__text__tfidf__lowercase', 'features__text__tfidf__max_df', 'features__text__tfidf__max_features', 'features__text__tfidf__min_df', 'features__text__tfidf__ngram_range', 'features__text__tfidf__norm', 'features__text__tfidf__preprocessor', 'features__text__tfidf__smooth_idf', 'features__text__tfidf__stop_words', 'features__text__tfidf__strip_accents', 'features__text__tfidf__sublinear_tf', 'features__text__tfidf__token_pattern', 'features__text__tfidf__tokenizer', 'features__text__tfidf__use_idf', 'features__text__tfidf__vocabulary', 'features__char_count__memory', 'features__char_count__steps', 'features__char_count__verbose', 'features__char_count__selector', 'features__char_count__standard', 'features__char_count__selector__key', 'features__char_count__standard__copy', 'features__char_count__standard__with_mean', 'features__char_count__standard__with_std', 'features__word_count__memory', 'features__word_count__steps', 'features__word_count__verbose', 'features__word_count__selector', 'features__word_count__standard', 'features__word_count__selector__key', 'features__word_count__standard__copy', 'features__word_count__standard__with_mean', 'features__word_count__standard__with_std', 'features__word_density__memory', 'features__word_density__steps', 'features__word_density__verbose', 'features__word_density__selector', 'features__word_density__standard', 'features__word_density__selector__key', 'features__word_density__standard__copy', 'features__word_density__standard__with_mean', 'features__word_density__standard__with_std', 'features__punctuation_count__memory', 'features__punctuation_count__steps', 'features__punctuation_count__verbose', 'features__punctuation_count__selector', 'features__punctuation_count__standard', 'features__punctuation_count__selector__key', 'features__punctuation_count__standard__copy', 'features__punctuation_count__standard__with_mean', 'features__punctuation_count__standard__with_std', 'features__title_word_count__memory', 'features__title_word_count__steps', 'features__title_word_count__verbose', 'features__title_word_count__selector', 'features__title_word_count__standard', 'features__title_word_count__selector__key', 'features__title_word_count__standard__copy', 'features__title_word_count__standard__with_mean', 'features__title_word_count__standard__with_std', 'features__upper_case_word_count__memory', 'features__upper_case_word_count__steps', 'features__upper_case_word_count__verbose', 'features__upper_case_word_count__selector', 'features__upper_case_word_count__standard', 'features__upper_case_word_count__selector__key', 'features__upper_case_word_count__standard__copy', 'features__upper_case_word_count__standard__with_mean', 'features__upper_case_word_count__standard__with_std', 'classifier__C', 'classifier__cache_size', 'classifier__class_weight', 'classifier__coef0', 'classifier__decision_function_shape', 'classifier__degree', 'classifier__gamma', 'classifier__kernel', 'classifier__max_iter', 'classifier__probability', 'classifier__random_state', 'classifier__shrinking', 'classifier__tol', 'classifier__verbose'])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('features',\n",
       "                                        FeatureUnion(n_jobs=None,\n",
       "                                                     transformer_list=[('text',\n",
       "                                                                        Pipeline(memory=None,\n",
       "                                                                                 steps=[('selector',\n",
       "                                                                                         TextSelector(key='text')),\n",
       "                                                                                        ('tfidf',\n",
       "                                                                                         TfidfVectorizer(analyzer='word',\n",
       "                                                                                                         binary=False,\n",
       "                                                                                                         decode_error='strict',\n",
       "                                                                                                         dtype=<class 'numpy.float64'>,\n",
       "                                                                                                         encoding='utf-8',\n",
       "                                                                                                         input='cont...\n",
       "                                            random_state=None, shrinking=True,\n",
       "                                            tol=0.001, verbose=False))],\n",
       "                                verbose=False),\n",
       "             iid='warn', n_jobs=None,\n",
       "             param_grid={'classifier__C': (0.01, 1, 10),\n",
       "                         'classifier__kernel': ('rbf', 'linear'),\n",
       "                         'features__text__tfidf__max_df': [0.9, 0.95],\n",
       "                         'features__text__tfidf__ngram_range': [(1, 1),\n",
       "                                                                (1, 2)]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "hyperparameters = { 'features__text__tfidf__max_df': [0.9, 0.95],\n",
    "                    'features__text__tfidf__ngram_range': [(1,1), (1,2)],\n",
    "                    'classifier__C': (0.01, 1, 10),\n",
    "                    #'clf__gamma: (0.5, 1,2,3,4),\n",
    "                    'classifier__kernel': ('rbf', 'linear')\n",
    "                  }\n",
    "clf = GridSearchCV(pipeline, hyperparameters, cv=5)\n",
    " \n",
    "# Fit and tune model\n",
    "clf.fit(ftrain_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 10,\n",
       " 'classifier__kernel': 'linear',\n",
       " 'features__text__tfidf__max_df': 0.9,\n",
       " 'features__text__tfidf__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
