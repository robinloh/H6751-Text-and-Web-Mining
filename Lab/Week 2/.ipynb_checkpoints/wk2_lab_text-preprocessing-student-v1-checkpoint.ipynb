{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing I (week 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab is prepared with the materials in the articles [Text Preprocessing in Python: Steps, Tools, and Examples]( https://www.kdnuggets.com/2018/11/text-preprocessing-python.html)\n",
    "and [Text Data Preprocessing: A Walkthrough in Python](https://www.kdnuggets.com/2018/03/text-data-preprocessing-walkthrough-python.html)\n",
    "\n",
    "We outline the basic steps of text preprocessing, which are needed for transferring text from human language to machine-readable format for further processing.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World!\n"
     ]
    }
   ],
   "source": [
    "print('Hello World!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Text data preprocessing: step by step approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert text to lowercase using lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 5 biggest countries by population in 2017 are china, india, united states, indonesia, and brazil.\n"
     ]
    }
   ],
   "source": [
    "input_str = \"The 5 biggest countries by population in 2017 are China, \\\n",
    "India, United States, Indonesia, and Brazil.\"\n",
    "input_str = input_str.lower()\n",
    "print(input_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "Tokenization is the process of splitting the given text into smaller pieces called tokens. Words, numbers, punctuation marks, and others can be considered as tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/robin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/robin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/robin/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/robin/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLTK', 'is', 'a', 'leading', 'platform', 'for', 'building', 'Python', 'programs', 'to', 'work', 'with', 'human', 'language', 'data', '.']\n",
      "\n",
      "['NLTK', 'is', 'a', 'leading', 'platform', 'for', 'building', 'Python', 'programs', 'to', 'work', 'with', 'human', 'language', 'data', '.']\n",
      "\n",
      "['They', \"'ll\", 'save', 'and', 'invest', 'more', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize # NLTK default tokenizer\n",
    "from nltk.tokenize import TreebankWordTokenizer \n",
    "\n",
    "# word_tokenize is an implementation of TreebankWordTokenizer \n",
    "s = \"NLTK is a leading platform for building Python programs to work with human language data.\" \n",
    "print(TreebankWordTokenizer().tokenize(s))  \n",
    "\n",
    "input_str = \"NLTK is a leading platform for building Python programs to work with human language data.\"\n",
    "tokens = word_tokenize(input_str)\n",
    "print()\n",
    "print (tokens) # output same as TreebankWordTokenizer\n",
    "\n",
    "s = \"They'll save and invest more.\" \n",
    "#split standard contractions - they'll => 'they', \"'ll\"\n",
    "print()\n",
    "print(word_tokenize(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['They', \"'\", 'll', 'save', 'and', 'invest', 'more', '.']\n",
      "[\"They'll\", 'save', 'and', 'invest', 'more.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer, WhitespaceTokenizer \n",
    "\n",
    "#Tokenize based on sequence of alphabetic (words) and non-alphabetic characters. \n",
    "print(WordPunctTokenizer().tokenize(s))\n",
    "\n",
    "#Tokenize based on whitespace\n",
    "print(WhitespaceTokenizer().tokenize(s)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stop words\n",
    "“Stop words” are the most common words in a language like “the”, “a”, “on”, “is”, “all”. These words do not carry important meaning and are usually removed from texts. It is possible to remove stop words using Natural Language Toolkit (NLTK), a suite of libraries and programs for symbolic and statistical natural language processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop Words:\n",
      "{'or', 'too', 'needn', 'its', 'here', 'once', 'were', \"you're\", \"you'd\", 'again', 'all', 'on', \"hadn't\", 'have', 'was', 'between', 'some', 't', 'about', 'shouldn', 'any', 'nor', 'ma', 'and', 'with', 'wouldn', 'so', \"she's\", 'that', 'been', 'doing', \"isn't\", \"that'll\", 'be', 's', 'your', \"wasn't\", \"mustn't\", 'to', 'who', 'by', 'when', 'these', \"mightn't\", 'hers', 'just', 'my', \"needn't\", 'their', 'doesn', 'can', 'ourselves', 'hasn', 'at', \"shouldn't\", 'don', 'a', 'if', 'until', 'through', 'off', 'only', 'as', 'under', 'couldn', 'than', 'there', 'yourself', 'they', 'own', 'won', 'in', \"should've\", 'an', 'out', 'below', 'up', 'such', 'hadn', 'had', 'mustn', \"shan't\", 'should', 'aren', 'during', 'being', 'didn', 'isn', \"doesn't\", 'does', 'i', \"weren't\", 'wasn', \"hasn't\", 'having', 've', 'why', 'into', 'but', 'further', 'not', 'you', 'ours', 'theirs', 'he', 'me', 'herself', 'where', 'them', \"wouldn't\", 'both', 'after', 'other', 'those', 'against', 'very', 'what', 'same', 'above', 'itself', 'his', \"haven't\", 'of', \"won't\", 'her', 'no', \"it's\", 'him', 'how', 'haven', 'few', \"aren't\", 'y', 're', 'yours', 'whom', 'is', 'most', 'will', 'now', 'myself', 'm', 'she', \"couldn't\", 'did', 'it', 'has', 'from', 'each', 'mightn', 'then', \"you've\", 'before', 'our', 'd', 'themselves', 'ain', 'which', \"didn't\", 'shan', \"don't\", 'for', 'more', 'while', 'yourselves', 'himself', 'we', 'are', 'because', 'am', 'o', 'll', 'weren', \"you'll\", 'this', 'down', 'do', 'the', 'over'}\n",
      "\n",
      "Results:\n",
      "['NLTK', 'leading', 'platform', 'building', 'Python', 'programs', 'work', 'human', 'language', 'data', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "input_str = \"NLTK is a leading platform for building \\\n",
    "Python programs to work with human language data.\"\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(\"Stop Words:\")\n",
    "print(stop_words)\n",
    "print()\n",
    "tokens = word_tokenize(input_str)\n",
    "result = [i for i in tokens if not i in stop_words]\n",
    "print(\"Results:\")\n",
    "print (result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "Stemming is a process of reducing words to their word stem, base or root form (for example, books — book, looked — look). The main two algorithms are Porter stemming algorithm (removes common morphological and inflexional endings from words) and Lancaster stemming algorithm (a more aggressive stemming algorithm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there\n",
      "are\n",
      "sever\n",
      "type\n",
      "of\n",
      "stem\n",
      "algorithm\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# use PortStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer= PorterStemmer()\n",
    "input_str=\"There are several types of stemming algorithms.\"\n",
    "input_str=word_tokenize(input_str)\n",
    "for word in input_str:\n",
    "    print(stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ther\n",
      "ar\n",
      "sev\n",
      "typ\n",
      "of\n",
      "stem\n",
      "algorithm\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# use LancasterStemmer - more aggressive\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "stemmer= LancasterStemmer()\n",
    "input_str=\"There are several types of stemming algorithms.\"\n",
    "input_str=word_tokenize(input_str)\n",
    "for word in input_str:\n",
    "    print(stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "The aim of lemmatization, like stemming, is to reduce inflectional forms to a common base form. As opposed to stemming, lemmatization does not simply chop off inflections. Instead it uses **lexical knowledge bases** to get the correct base forms of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "been\n",
      "had\n",
      "done\n",
      "language\n",
      "city\n",
      "mouse\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "input_str=\"been had done languages cities mice\"\n",
    "input_str=word_tokenize(input_str)\n",
    "for word in input_str:\n",
    "    print(lemmatizer.lemmatize(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of sentences  =  5\n",
      "[\"this's a sent tokenize test.\", 'this is sent two.', 'is this sent three?', 'sent 4 is cool!', \"Now it's your turn.\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"this's a sent tokenize test. this is sent two. is this sent three? sent 4 is cool! Now it's your turn.\"\n",
    "sent_tokenize_list = sent_tokenize(text)\n",
    "\n",
    "print(\"The length of sentences\", \" = \", len(sent_tokenize_list))\n",
    "print(sent_tokenize_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_preprocess(document):\n",
    "    sentences = nltk.sent_tokenize(document) \n",
    "    print(sentences, \"\\n\")\n",
    "    sentences = [nltk.word_tokenize(sent) for sent in sentences] \n",
    "    print(sentences, \"\\n\")\n",
    "    sentences = [nltk.pos_tag(sent) for sent in sentences] \n",
    "    print(sentences, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_doc = \"\"\"I cannot say I like the movie. The acting is really lousy, don't you think so?\n",
    "\n",
    "My favorite movie franchises, in order are: (1) Indiana Jones; (2) Marvel Cinematic Universe; (3) Star Wars; (4) Back to the Future.\n",
    "\n",
    "This is a great movie by Tom Cruise.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I cannot say I like the movie.', \"The acting is really lousy, don't you think so?\", 'My favorite movie franchises, in order are: (1) Indiana Jones; (2) Marvel Cinematic Universe; (3) Star Wars; (4) Back to the Future.', 'This is a great movie by Tom Cruise.'] \n",
      "\n",
      "[['I', 'can', 'not', 'say', 'I', 'like', 'the', 'movie', '.'], ['The', 'acting', 'is', 'really', 'lousy', ',', 'do', \"n't\", 'you', 'think', 'so', '?'], ['My', 'favorite', 'movie', 'franchises', ',', 'in', 'order', 'are', ':', '(', '1', ')', 'Indiana', 'Jones', ';', '(', '2', ')', 'Marvel', 'Cinematic', 'Universe', ';', '(', '3', ')', 'Star', 'Wars', ';', '(', '4', ')', 'Back', 'to', 'the', 'Future', '.'], ['This', 'is', 'a', 'great', 'movie', 'by', 'Tom', 'Cruise', '.']] \n",
      "\n",
      "[[('I', 'PRP'), ('can', 'MD'), ('not', 'RB'), ('say', 'VB'), ('I', 'PRP'), ('like', 'IN'), ('the', 'DT'), ('movie', 'NN'), ('.', '.')], [('The', 'DT'), ('acting', 'NN'), ('is', 'VBZ'), ('really', 'RB'), ('lousy', 'JJ'), (',', ','), ('do', 'VBP'), (\"n't\", 'RB'), ('you', 'PRP'), ('think', 'VB'), ('so', 'RB'), ('?', '.')], [('My', 'PRP$'), ('favorite', 'JJ'), ('movie', 'NN'), ('franchises', 'NNS'), (',', ','), ('in', 'IN'), ('order', 'NN'), ('are', 'VBP'), (':', ':'), ('(', '('), ('1', 'CD'), (')', ')'), ('Indiana', 'NNP'), ('Jones', 'NNP'), (';', ':'), ('(', '('), ('2', 'CD'), (')', ')'), ('Marvel', 'NNP'), ('Cinematic', 'NNP'), ('Universe', 'NNP'), (';', ':'), ('(', '('), ('3', 'CD'), (')', ')'), ('Star', 'NNP'), ('Wars', 'NNP'), (';', ':'), ('(', '('), ('4', 'CD'), (')', ')'), ('Back', 'RB'), ('to', 'TO'), ('the', 'DT'), ('Future', 'NNP'), ('.', '.')], [('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('great', 'JJ'), ('movie', 'NN'), ('by', 'IN'), ('Tom', 'NNP'), ('Cruise', 'NNP'), ('.', '.')]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence_preprocess(sample_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Additional Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n-grams\n",
    "The TextBlob.ngrams() method returns a list of tuples of n successive words.\n",
    "[TextBlob](https://textblob.readthedocs.io/en/dev/) is a Python library for processing textual data. It provides a simple API for diving into common natural language processing (NLP) tasks such as part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, translation, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 won't be maintained after that date. A future version of pip will drop support for Python 2.7. More details about Python 2 support in pip, can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support\u001b[0m\n",
      "Requirement already satisfied: TextBlob in /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages (0.15.3)\n",
      "Requirement already satisfied: nltk>=3.1 in /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages (from TextBlob) (3.4.5)\n",
      "Requirement already satisfied: six in /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages (from nltk>=3.1->TextBlob) (1.12.0)\n",
      "Requirement already satisfied: singledispatch in /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages (from nltk>=3.1->TextBlob) (3.4.0.3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[WordList(['Now', 'is']),\n",
       " WordList(['is', 'better']),\n",
       " WordList(['better', 'than']),\n",
       " WordList(['than', 'never'])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use TextBlob\n",
    "!pip install TextBlob\n",
    "from textblob import TextBlob\n",
    "blob = TextBlob(\"Now is better than never.\")\n",
    "blob.ngrams(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['Now', 'is', 'better']),\n",
       " WordList(['is', 'better', 'than']),\n",
       " WordList(['better', 'than', 'never'])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.ngrams(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NLTK', 'is'),\n",
       " ('is', 'a'),\n",
       " ('a', 'leading'),\n",
       " ('leading', 'platform'),\n",
       " ('platform', 'for'),\n",
       " ('for', 'building'),\n",
       " ('building', 'Python'),\n",
       " ('Python', 'programs'),\n",
       " ('programs', 'to'),\n",
       " ('to', 'work'),\n",
       " ('work', 'with'),\n",
       " ('with', 'human'),\n",
       " ('human', 'language'),\n",
       " ('language', 'data'),\n",
       " ('data', '.')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use nltk\n",
    "from nltk.util import ngrams, bigrams\n",
    "input_str = \"NLTK is a leading platform for building Python programs to work with human language data.\"\n",
    "tokens = word_tokenize(input_str)\n",
    "list(ngrams(tokens, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NLTK', 'is'),\n",
       " ('is', 'a'),\n",
       " ('a', 'leading'),\n",
       " ('leading', 'platform'),\n",
       " ('platform', 'for'),\n",
       " ('for', 'building'),\n",
       " ('building', 'Python'),\n",
       " ('Python', 'programs'),\n",
       " ('programs', 'to'),\n",
       " ('to', 'work'),\n",
       " ('work', 'with'),\n",
       " ('with', 'human'),\n",
       " ('human', 'language'),\n",
       " ('language', 'data'),\n",
       " ('data', '.')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(bigrams(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NLTK', 'is', 'a'),\n",
       " ('is', 'a', 'leading'),\n",
       " ('a', 'leading', 'platform'),\n",
       " ('leading', 'platform', 'for'),\n",
       " ('platform', 'for', 'building'),\n",
       " ('for', 'building', 'Python'),\n",
       " ('building', 'Python', 'programs'),\n",
       " ('Python', 'programs', 'to'),\n",
       " ('programs', 'to', 'work'),\n",
       " ('to', 'work', 'with'),\n",
       " ('work', 'with', 'human'),\n",
       " ('with', 'human', 'language'),\n",
       " ('human', 'language', 'data'),\n",
       " ('language', 'data', '.')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams(tokens, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find most common ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('one', 'of', 'the'), 332),\n",
       " (('out', 'of', 'the'), 244),\n",
       " (('of', 'the', 'United'), 235),\n",
       " (('that', 'he', 'was'), 191),\n",
       " (('the', 'United', 'States'), 184),\n",
       " (('that', 'it', 'was'), 180),\n",
       " (('and', 'in', 'the'), 174),\n",
       " (('met', 'with', 'in'), 173),\n",
       " (('up', 'to', 'the'), 159),\n",
       " (('part', 'of', 'the'), 158)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from nltk import ngrams\n",
    "bigtxt = open('big.txt').read()\n",
    "ngram_counts = Counter(ngrams(bigtxt.split(), 3))\n",
    "ngram_counts.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('Received:', 'from'), 12579),\n",
       " (('with', 'ESMTP'), 7188),\n",
       " (('ESMTP', 'id'), 7188),\n",
       " (('Dec', '2007'), 7063),\n",
       " (('Nov', '2007'), 6810),\n",
       " (('-0500', 'Received:'), 5843),\n",
       " (('for', '<source@collab.sakaiproject.org>;'), 5391),\n",
       " (('text/plain;', 'charset=UTF-8'), 5391),\n",
       " (('+0000', '(GMT)'), 4932),\n",
       " (('from', 'murder'), 3594)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigtxt = open('mbox.txt').read()\n",
    "ngram_counts = Counter(ngrams(bigtxt.split(), 2))\n",
    "ngram_counts.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordNet\n",
    "WordNet is a semantically-oriented dictionary of English, similar to a traditional thesaurus but with a richer structure. NLTK includes the English WordNet, with 155,287 words and 117,659 synonym sets. We'll begin by looking at synonyms and how they are accessed in WordNet.\n",
    "Reference: https://www.nltk.org/book/ch02.html and http://www.nltk.org/howto/wordnet.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('car.n.01')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "wn.synsets('motorcar') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['car', 'auto', 'automobile', 'machine', 'motorcar']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The entity car.n.01 is called a synset, or \"synonym set\", a collection of synonymous words (or \"lemmas\")\n",
    "wn.synset('car.n.01').lemma_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a motor vehicle with four wheels; usually propelled by an internal combustion engine'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('car.n.01').definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['he needs a car to get to work']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('car.n.01').examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('motor_vehicle.n.01')]\n"
     ]
    }
   ],
   "source": [
    "motorcar = wn.synset('car.n.01')\n",
    "# hypernyms - parent synsets with broader meaning\n",
    "types_of_motorcar = motorcar.hypernyms()  \n",
    "print(types_of_motorcar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('burl.n.02'),\n",
       " Synset('crown.n.07'),\n",
       " Synset('limb.n.02'),\n",
       " Synset('stump.n.01'),\n",
       " Synset('trunk.n.01')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from an item to its components (meronyms) \n",
    "wn.synset('tree.n.01').part_meronyms() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('demand.n.02.demand')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.lemma('supply.n.02.supply').antonyms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exercise - Text data preprocessing with a sample text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_doc = \"\"\"I cannot say I like the movie. The acting is really lousy, don't you think so?\n",
    "\n",
    "My favorite movie franchises, in order are: (1) Indiana Jones; (2) Marvel Cinematic Universe; (3) Star Wars; (4) Back to the Future.\n",
    "\n",
    "This is a great movie by Tom Cruise.\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of sentences  =  4\n",
      "['I cannot say I like the movie.', \"The acting is really lousy, don't you think so?\", 'My favorite movie franchises, in order are: (1) Indiana Jones; (2) Marvel Cinematic Universe; (3) Star Wars; (4) Back to the Future.', 'This is a great movie by Tom Cruise.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sent_tokenize_list = sent_tokenize(sample_doc)\n",
    "\n",
    "print(\"The length of sentences\", \" = \", len(sent_tokenize_list))\n",
    "print(sent_tokenize_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'can', 'not', 'say', 'I', 'like', 'the', 'movie', '.', 'The', 'acting', 'is', 'really', 'lousy', ',', 'do', \"n't\", 'you', 'think', 'so', '?', 'My', 'favorite', 'movie', 'franchises', ',', 'in', 'order', 'are', ':', '(', '1', ')', 'Indiana', 'Jones', ';', '(', '2', ')', 'Marvel', 'Cinematic', 'Universe', ';', '(', '3', ')', 'Star', 'Wars', ';', '(', '4', ')', 'Back', 'to', 'the', 'Future', '.', 'This', 'is', 'a', 'great', 'movie', 'by', 'Tom', 'Cruise', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize # NLTK default tokenizer\n",
    "words = nltk.word_tokenize(sample_doc) ### Should we tokenize document or sentence level?\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 won't be maintained after that date. A future version of pip will drop support for Python 2.7. More details about Python 2 support in pip, can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support\u001b[0m\n",
      "Requirement already satisfied: inflect in /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages (2.1.0)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'inflect'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-fea544b2f1d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0minflect\u001b[0m \u001b[0;31m# usage - convert integers to text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'inflect'"
     ]
    }
   ],
   "source": [
    "!pip install inflect \n",
    "\n",
    "import re\n",
    "import inflect # usage - convert integers to text\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "print(words)\n",
    "print()\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word) \n",
    "        # \\w: an alphanumeric character; \\s: a whitespace character\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def replace_numbers(words):\n",
    "    \"\"\"Replace all interger occurrences in list of tokenized words \\\n",
    "    with textual representation\"\"\"\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            new_word = p.number_to_words(word)\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def normalize(words):\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = replace_numbers(words)\n",
    "    words = remove_stopwords(words)\n",
    "    return words\n",
    "\n",
    "words = normalize(words)\n",
    "\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming and lemming functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def stem_words(words):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def stem_and_lemmatize(words):\n",
    "    stems = stem_words(words)\n",
    "    lemmas = lemmatize_verbs(words)\n",
    "    return stems, lemmas\n",
    "\n",
    "stems, lemmas = stem_and_lemmatize(words)\n",
    "\n",
    "print('Stemmed:\\n', stems)\n",
    "print('\\nLemmatized:\\n', lemmas)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
