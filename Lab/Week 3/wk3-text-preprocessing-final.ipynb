{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing II (week 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordNet - continuation from week 2\n",
    "WordNet is a semantically-oriented dictionary of English, similar to a traditional thesaurus but with a richer structure. NLTK includes the English WordNet, with 155,287 words and 117,659 synonym sets. We'll begin by looking at synonyms and how they are accessed in WordNet.\n",
    "Reference: https://www.nltk.org/book/ch02.html and http://www.nltk.org/howto/wordnet.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('car.n.01')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "wn.synsets('motorcar') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['car', 'auto', 'automobile', 'machine', 'motorcar']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The entity car.n.01 is called a synset, or \"synonym set\", a collection of synonymous words (or \"lemmas\")\n",
    "wn.synset('car.n.01').lemma_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a motor vehicle with four wheels; usually propelled by an internal combustion engine'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('car.n.01').definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['he needs a car to get to work']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('car.n.01').examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('motor_vehicle.n.01')]\n"
     ]
    }
   ],
   "source": [
    "motorcar = wn.synset('car.n.01')\n",
    "# hypernyms - parent synsets with broader meaning\n",
    "types_of_motorcar = motorcar.hypernyms()  \n",
    "print(types_of_motorcar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('burl.n.02'),\n",
       " Synset('crown.n.07'),\n",
       " Synset('limb.n.02'),\n",
       " Synset('stump.n.01'),\n",
       " Synset('trunk.n.01')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from an item to its components (meronyms) \n",
    "wn.synset('tree.n.01').part_meronyms() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('demand.n.02.demand')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.lemma('supply.n.02.supply').antonyms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 3\n",
    "### Word Sense Disambiguation\n",
    "\n",
    "Lesk Algorithm (http://www.nltk.org/howto/wsd.html; https://www.nltk.org/_modules/nltk/wsd.html) performs the classic Lesk algorithm for Word Sense Disambiguation (WSD) using the definitions of the ambiguous word. Given an ambiguous word and the context in which the word occurs, Lesk returns a Synset with the highest number of overlapping words between the context sentence and different definitions from each Synset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('savings_bank.n.02')\n",
      "Synset('savings_bank.n.02')\n"
     ]
    }
   ],
   "source": [
    "from nltk.wsd import lesk\n",
    "sent = ['I', 'went', 'to', 'the', 'bank', 'to', 'deposit', 'money', '.']\n",
    "print(lesk(sent, 'bank', 'n'))\n",
    "print(lesk(sent, 'bank'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('bank.n.01') sloping land (especially the slope beside a body of water)\n",
      "Synset('depository_financial_institution.n.01') a financial institution that accepts deposits and channels the money into lending activities\n",
      "Synset('bank.n.03') a long ridge or pile\n",
      "Synset('bank.n.04') an arrangement of similar objects in a row or in tiers\n",
      "Synset('bank.n.05') a supply or stock held in reserve for future use (especially in emergencies)\n",
      "Synset('bank.n.06') the funds held by a gambling house or the dealer in some gambling games\n",
      "Synset('bank.n.07') a slope in the turn of a road or track; the outside is higher than the inside in order to reduce the effects of centrifugal force\n",
      "Synset('savings_bank.n.02') a container (usually with a slot in the top) for keeping money at home\n",
      "Synset('bank.n.09') a building in which the business of banking transacted\n",
      "Synset('bank.n.10') a flight maneuver; aircraft tips laterally about its longitudinal axis (especially in turning)\n",
      "Synset('bank.v.01') tip laterally\n",
      "Synset('bank.v.02') enclose with a bank\n",
      "Synset('bank.v.03') do business with a bank or keep an account at a bank\n",
      "Synset('bank.v.04') act as the banker in a game or in gambling\n",
      "Synset('bank.v.05') be in the banking business\n",
      "Synset('deposit.v.02') put into a bank account\n",
      "Synset('bank.v.07') cover with ashes so to control the rate of burning\n",
      "Synset('trust.v.01') have confidence or faith in\n"
     ]
    }
   ],
   "source": [
    "# The definitions for \"bank\" are:\n",
    "# online version: http://wordnetweb.princeton.edu/perl/webwn\n",
    "# \"bank\" in sent is close to the sense of \n",
    "# Synset('depository_financial_institution.n.01'). \n",
    "from nltk.corpus import wordnet as wn\n",
    "for ss in wn.synsets('bank'):\n",
    "    print(ss, ss.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(Synset('able.a.01'), 'a'), (Synset('able.s.02'), 's'), (Synset('able.s.03'), 's'), (Synset('able.s.04'), 's')]\n",
      "Synset('able.s.04')\n",
      "Synset('able.a.01')\n"
     ]
    }
   ],
   "source": [
    "# Test disambiguation of POS tagged \"able\". - more accurate result\n",
    "print([(s, s.pos()) for s in wn.synsets('able')])\n",
    "sent = 'people should be able to marry a person of their choice'.split()\n",
    "print(lesk(sent, 'able'))\n",
    "print(lesk(sent, 'able', pos='a'))  # provide a correct synset\n",
    "# a means ADJECTIVE; s means ADJECTIVE SATELLITE \n",
    "# Certain adjectives bind minimal meaning. e.g. \"dry\", \"good\", etc. Each of these is the center of an adjective synset in WN.\n",
    "# Adjective satellites imposes additional commitments on top of the meaning of the central adjective, \n",
    "# e.g. \"arid\" = \"dry\" + a particular context (i.e. climates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('able.a.01') (usually followed by `to') having the necessary means or skill or know-how or authority to do something\n",
      "Synset('able.s.02') have the skills and qualifications to do things well\n",
      "Synset('able.s.03') having inherent physical or mental ability or capacity\n",
      "Synset('able.s.04') having a strong healthy body\n"
     ]
    }
   ],
   "source": [
    "for ss in wn.synsets('able'):\n",
    "    print(ss, ss.definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise Removal\n",
    " \n",
    "Many denoising tasks, such as removing HTML markups, parsing a JSON structure, would need to be implemented prior to tokenization. In our data preprocessing pipeline, we will strip away HTML markup with the help of the BeautifulSoup library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = \"\"\"<h1>Title Goes Here</h1>\n",
    "\n",
    "<b>Bolded Text</b>\n",
    "<i>Italicized Text</i>\n",
    "\n",
    "<img src=\"this should all be gone\"/>\n",
    "<a href=\"this will be gone, too\">But this will still be here!</a>\n",
    "\n",
    "I run. He ran. She is running. Will they stop running?\n",
    "I talked. She was talking. They talked to them about running. Who ran to the talking runner?\n",
    "\n",
    "[Some text we don't want to keep is in here]\n",
    "\n",
    "¡Sebastián, Nicolás, Alejandro and Jéronimo are going to the store tomorrow morning!\n",
    "\n",
    "something... is! wrong() with.,; this :: sentence.\n",
    "\n",
    "I can't do this anymore. I didn't know them. Why couldn't you have dinner at the restaurant?\n",
    "\n",
    "My favorite movie franchises, in order: Indiana Jones; Marvel Cinematic Universe; Star Wars; Back to the Future; Harry Potter.\n",
    "\n",
    "Don't do it.... Just don't. Billy! I know what you're doing. This is a great little house you've got here.\n",
    "\n",
    "[This is some other unwanted text]\n",
    "\n",
    "John: \"Well, well, well.\"\n",
    "James: \"There, there. There, there.\"\n",
    "\n",
    "&nbsp;&nbsp;\n",
    "\n",
    "There are a lot of reasons not to do this. There are 101 reasons not to do it. 1000000 reasons, actually.\n",
    "\n",
    "I have to go get 2 tutus from 2 different stores, too.\n",
    "\n",
    "22    45   1067   445\n",
    "\n",
    "{{Here is some stuff inside of double curly braces.}}\n",
    "\n",
    "{Here is more stuff in single curly braces.}\n",
    "\n",
    "[DELETE]\n",
    "\n",
    "</body>\n",
    "</html>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title Goes Here\n",
      "Bolded Text\n",
      "Italicized Text\n",
      "\n",
      "But this will still be here!\n",
      "\n",
      "I run. He ran. She is running. Will they stop running?\n",
      "I talked. She was talking. They talked to them about running. Who ran to the talking runner?\n",
      "\n",
      "[Some text we don't want to keep is in here]\n",
      "\n",
      "¡Sebastián, Nicolás, Alejandro and Jéronimo are going to the store tomorrow morning!\n",
      "\n",
      "something... is! wrong() with.,; this :: sentence.\n",
      "\n",
      "I can't do this anymore. I didn't know them. Why couldn't you have dinner at the restaurant?\n",
      "\n",
      "My favorite movie franchises, in order: Indiana Jones; Marvel Cinematic Universe; Star Wars; Back to the Future; Harry Potter.\n",
      "\n",
      "Don't do it.... Just don't. Billy! I know what you're doing. This is a great little house you've got here.\n",
      "\n",
      "[This is some other unwanted text]\n",
      "\n",
      "John: \"Well, well, well.\"\n",
      "James: \"There, there. There, there.\"\n",
      "\n",
      "  \n",
      "\n",
      "There are a lot of reasons not to do this. There are 101 reasons not to do it. 1000000 reasons, actually.\n",
      "\n",
      "I have to go get 2 tutus from 2 different stores, too.\n",
      "\n",
      "22    45   1067   445\n",
      "\n",
      "{{Here is some stuff inside of double curly braces.}}\n",
      "\n",
      "{Here is more stuff in single curly braces.}\n",
      "\n",
      "[DELETE]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "sample = strip_html(sample)\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expanding contractions\n",
    "While not mandatory to do at this stage prior to tokenization (you'll find that this statement is the norm for the relatively flexible ordering of text data preprocessing tasks), replacing contractions with their expansions can be beneficial at this point, since our word tokenizer will split words like \"didn't\" into \"did\" and \"n't.\" It's not impossible to remedy this tokenization at a later stage, but doing so prior makes it easier and more straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contractions in c:\\anaconda3\\lib\\site-packages (0.0.21)\n",
      "I can not go to the movies. We do not want to buy the books.\n"
     ]
    }
   ],
   "source": [
    "!pip install contractions\n",
    "import contractions\n",
    "\n",
    "text = \"I can't go to the movies. We don't want to buy the books.\"\n",
    "\n",
    "output = contractions.fix(text)  # e.g., can't -> cannot; don't -> do not\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title Goes Here\n",
      "Bolded Text\n",
      "Italicized Text\n",
      "\n",
      "But this will still be here!\n",
      "\n",
      "I run. He ran. She is running. Will they stop running?\n",
      "I talked. She was talking. They talked to them about running. Who ran to the talking runner?\n",
      "\n",
      "[Some text we do not want to keep is in here]\n",
      "\n",
      "¡Sebastián, Nicolás, Alejandro and Jéronimo are going to the store tomorrow morning!\n",
      "\n",
      "something... is! wrong() with.,; this :: sentence.\n",
      "\n",
      "I can not do this anymore. I did not know them. Why could not you have dinner at the restaurant?\n",
      "\n",
      "My favorite movie franchises, in order: Indiana Jones; Marvel Cinematic Universe; Star Wars; Back to the Future; Harry Potter.\n",
      "\n",
      "do not do it.... Just do not. Billy! I know what you are doing. This is a great little house you have got here.\n",
      "\n",
      "[This is some other unwanted text]\n",
      "\n",
      "John: \"Well, well, well.\"\n",
      "James: \"There, there. There, there.\"\n",
      "\n",
      "  \n",
      "\n",
      "There are a lot of reasons not to do this. There are 101 reasons not to do it. 1000000 reasons, actually.\n",
      "\n",
      "I have to go get 2 tutus from 2 different stores, too.\n",
      "\n",
      "22    45   1067   445\n",
      "\n",
      "{{Here is some stuff inside of double curly braces.}}\n",
      "\n",
      "{Here is more stuff in single curly braces.}\n",
      "\n",
      "[DELETE]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def replace_contractions(text):\n",
    "    \"\"\"Replace contractions in string of text\"\"\"\n",
    "    return contractions.fix(text)\n",
    "\n",
    "sample = replace_contractions(sample)  \n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Expression\n",
    "\n",
    "Many linguistic processing tasks involve pattern matching. For example, we can find words ending with ed using endswith('ed'). Regular expressions give us a more powerful and flexible method for describing the character patterns we are interested in.\n",
    "Refer to 3.4   Regular Expressions for Detecting Word Patterns (https://www.nltk.org/book/ch03.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['groundhog', 'Groundhog', 'Woodchuck', 'woodchuck']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "x = 'groundhog Groundhog Woodchuck woodchuck'\n",
    "y = re.findall('[gG]roundhog|[Ww]oodchuck',x)\n",
    "print (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2', '19', '42']\n"
     ]
    }
   ],
   "source": [
    "x = 'My 2 favorite numbers are 19 and 42'\n",
    "y = re.findall('[0-9]+',x) # [ ] matches only one character, + => one or more\n",
    "print (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['456']\n"
     ]
    }
   ],
   "source": [
    "x = '1234567890'\n",
    "y = re.findall('(456)',x) # ( ) matches all characters as a group\n",
    "print (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "['$10.00']\n"
     ]
    }
   ],
   "source": [
    "x = 'We just received $10.00 for cookies.'\n",
    "y = re.findall('$[0-9.]+',x) # $ indicates end of string/line. \n",
    "print(y)\n",
    "y = re.findall('\\$[0-9.]+',x) # \\ => escape special character $\n",
    "print (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['support@abc.com', 'sales@ABC.com']\n"
     ]
    }
   ],
   "source": [
    "x = 'Please contact us at: support@abc.com, sales@ABC.com'\n",
    "y = re.findall('[\\w]+@[\\w.]+',x) # \n",
    "print (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b[a-cA-C]+.com\b\n",
      "[]\n",
      "['abc.com', 'ABC.com']\n",
      "['abc.com', 'ABC.com']\n"
     ]
    }
   ],
   "source": [
    "# \\b is a word boundary in regex and backspace in python\n",
    "y = '\\b[a-cA-C]+.com\\b'\n",
    "print(y)\n",
    "#input to regex is [a-cA-C]+.co => no match\n",
    "y = re.findall('\\b[a-cA-C]+.com\\b',x)    \n",
    "print(y)\n",
    "\n",
    "# backslash \\ => an escape sequence. \\\\b causes regex input to be \\b   \n",
    "y = re.findall('\\\\b[a-cA-C]+.com\\\\b',x) \n",
    "print (y)\n",
    "\n",
    "# r at start of pattern interprets string as raw - treats \\b as literal \n",
    "y = re.findall(r'\\b[a-cA-C]+.com\\b',x) \n",
    "print (y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['From: Using the :']\n"
     ]
    }
   ],
   "source": [
    "x = 'From: Using the : character'\n",
    "y = re.findall('^F.+:', x)\n",
    "print (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['From:']\n"
     ]
    }
   ],
   "source": [
    "# ? meant for non-greedy matching - stops at first : match\n",
    "y = re.findall('^F.+?:', x)  \n",
    "print (y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of speech tagging (POS)\n",
    "Part-of-speech tagging aims to assign parts of speech to each word of a given text (such as nouns, verbs, adjectives, and others) based on its definition and its context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\audi7\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\audi7\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\audi7\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Parts', 'NNS'), ('of', 'IN'), ('speech', 'NN'), ('examples', 'NNS'), (':', ':'), ('an', 'DT'), ('article', 'NN'), (',', ','), ('to', 'TO'), ('write', 'VB'), (',', ','), ('interesting', 'VBG'), (',', ','), ('easily', 'RB'), (',', ','), ('and', 'CC'), (',', ','), ('of', 'IN')]\n"
     ]
    }
   ],
   "source": [
    "### POS TAGGING using nltk ###\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = word_tokenize(\"Parts of speech examples: an article, to write, interesting, easily, and, of\")\n",
    "print(nltk.pos_tag(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TextBlob](https://textblob.readthedocs.io/en/dev/) is a Python library for processing textual data. It provides a simple API for diving into common natural language processing (NLP) tasks such as part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, translation, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Parts', 'NNS'), ('of', 'IN'), ('speech', 'NN'), ('examples', 'NNS'), ('an', 'DT'), ('article', 'NN'), ('to', 'TO'), ('write', 'VB'), ('interesting', 'VBG'), ('easily', 'RB'), ('and', 'CC'), ('of', 'IN')]\n"
     ]
    }
   ],
   "source": [
    "### POS TAGGING using TextBlob - omit punctuations ### \n",
    "input_str=\"Parts of speech examples: an article, to write, interesting, easily, and, of\"\n",
    "from textblob import TextBlob\n",
    "result = TextBlob(input_str)\n",
    "print(result.tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking (shallow parsing)\n",
    "Chunking is a natural language process that identifies constituent parts of sentences (nouns, verbs, adjectives, etc.) and links them to higher order units that have discrete grammatical meanings (**noun groups or phrases, verb groups**, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('A', 'DT'), ('black', 'JJ'), ('television', 'NN'), ('and', 'CC'), ('a', 'DT'), ('white', 'JJ'), ('stove', 'NN'), ('were', 'VBD'), ('bought', 'VBN'), ('for', 'IN'), ('the', 'DT'), ('new', 'JJ'), ('apartment', 'NN'), ('of', 'IN'), ('John', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "# The first step is to determine the part of speech for each word:\n",
    "input_str=\"A black television and a white stove were bought for the new apartment of John.\"\n",
    "result = TextBlob(input_str)\n",
    "print(result.tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['black television', 'white stove', 'new apartment', 'john'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract noun phrases using TextBlob\n",
    "# need to install necessary data: $> python -m textblob.download_corpora\n",
    "result.noun_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP A/DT black/JJ television/NN)\n",
      "  and/CC\n",
      "  (NP a/DT white/JJ stove/NN)\n",
      "  were/VBD\n",
      "  bought/VBN\n",
      "  for/IN\n",
      "  (NP the/DT new/JJ apartment/NN)\n",
      "  of/IN\n",
      "  John/NNP)\n"
     ]
    }
   ],
   "source": [
    "# extract noun phrases using NLTK\n",
    "# refer to \"7. Extracting Information from Text\" (https://www.nltk.org/book/ch07.html)\n",
    "reg_exp = \"NP: {<DT>?<JJ>*<NN>}\"   # need to define gramma (det, adj, nn) before parsing a sentence.\n",
    "rp = nltk.RegexpParser(reg_exp)\n",
    "result1 = rp.parse(result.tags) #parse words with POS\n",
    "print(result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "result1.draw() # result on pop-up window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP A/DT black/JJ television/NN)\n",
      "  and/CC\n",
      "  (NP a/DT white/JJ stove/NN)\n",
      "  were/VBD\n",
      "  (VP\n",
      "    bought/VBN\n",
      "    (PP for/IN (NP the/DT new/JJ apartment/NN))\n",
      "    (PP of/IN (NP John/NNP))))\n"
     ]
    }
   ],
   "source": [
    "# Define grammar before parsing\n",
    "grammar = r\"\"\"\n",
    "  NP: {<DT|JJ|NN.*>+}          # Chunk sequences of DT, JJ, NN\n",
    "  PP: {<IN><NP>}               # Chunk prepositions followed by NP\n",
    "  VP: {<VB.*><NP|PP|CLAUSE>+$} # Chunk verbs and their arguments\n",
    "  CLAUSE: {<NP><VP>}           # Chunk NP, VP\n",
    "  \"\"\"\n",
    "rp = nltk.RegexpParser(grammar)\n",
    "result2 = rp.parse(result.tags)\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2.draw() # result on pop-up window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n-grams\n",
    "The TextBlob.ngrams() method returns a list of tuples of n successive words.\n",
    "[TextBlob](https://textblob.readthedocs.io/en/dev/) is a Python library for processing textual data. It provides a simple API for diving into common natural language processing (NLP) tasks such as part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, translation, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: TextBlob in c:\\anaconda3\\lib\\site-packages (0.15.3)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\anaconda3\\lib\\site-packages (from TextBlob) (3.4)\n",
      "Requirement already satisfied: six in c:\\anaconda3\\lib\\site-packages (from nltk>=3.1->TextBlob) (1.12.0)\n",
      "Requirement already satisfied: singledispatch in c:\\anaconda3\\lib\\site-packages (from nltk>=3.1->TextBlob) (3.4.0.3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[WordList(['Now', 'is']),\n",
       " WordList(['is', 'better']),\n",
       " WordList(['better', 'than']),\n",
       " WordList(['than', 'never'])]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use TextBlob\n",
    "!pip install TextBlob\n",
    "from textblob import TextBlob\n",
    "blob = TextBlob(\"Now is better than never.\")\n",
    "blob.ngrams(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['Now', 'is', 'better']),\n",
       " WordList(['is', 'better', 'than']),\n",
       " WordList(['better', 'than', 'never'])]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.ngrams(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NLTK', 'is'),\n",
       " ('is', 'a'),\n",
       " ('a', 'leading'),\n",
       " ('leading', 'platform'),\n",
       " ('platform', 'for'),\n",
       " ('for', 'building'),\n",
       " ('building', 'Python'),\n",
       " ('Python', 'programs'),\n",
       " ('programs', 'to'),\n",
       " ('to', 'work'),\n",
       " ('work', 'with'),\n",
       " ('with', 'human'),\n",
       " ('human', 'language'),\n",
       " ('language', 'data'),\n",
       " ('data', '.')]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use nltk\n",
    "from nltk.util import ngrams, bigrams\n",
    "input_str = \"NLTK is a leading platform for building Python programs to work with human language data.\"\n",
    "tokens = word_tokenize(input_str)\n",
    "list(ngrams(tokens, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NLTK', 'is'),\n",
       " ('is', 'a'),\n",
       " ('a', 'leading'),\n",
       " ('leading', 'platform'),\n",
       " ('platform', 'for'),\n",
       " ('for', 'building'),\n",
       " ('building', 'Python'),\n",
       " ('Python', 'programs'),\n",
       " ('programs', 'to'),\n",
       " ('to', 'work'),\n",
       " ('work', 'with'),\n",
       " ('with', 'human'),\n",
       " ('human', 'language'),\n",
       " ('language', 'data'),\n",
       " ('data', '.')]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(bigrams(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NLTK', 'is', 'a'),\n",
       " ('is', 'a', 'leading'),\n",
       " ('a', 'leading', 'platform'),\n",
       " ('leading', 'platform', 'for'),\n",
       " ('platform', 'for', 'building'),\n",
       " ('for', 'building', 'Python'),\n",
       " ('building', 'Python', 'programs'),\n",
       " ('Python', 'programs', 'to'),\n",
       " ('programs', 'to', 'work'),\n",
       " ('to', 'work', 'with'),\n",
       " ('work', 'with', 'human'),\n",
       " ('with', 'human', 'language'),\n",
       " ('human', 'language', 'data'),\n",
       " ('language', 'data', '.')]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams(tokens, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find most common ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('one', 'of', 'the'), 332),\n",
       " (('out', 'of', 'the'), 244),\n",
       " (('of', 'the', 'United'), 235),\n",
       " (('that', 'he', 'was'), 191),\n",
       " (('the', 'United', 'States'), 184),\n",
       " (('that', 'it', 'was'), 180),\n",
       " (('and', 'in', 'the'), 174),\n",
       " (('met', 'with', 'in'), 173),\n",
       " (('up', 'to', 'the'), 159),\n",
       " (('part', 'of', 'the'), 158)]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from nltk import ngrams\n",
    "bigtxt = open('big.txt').read()\n",
    "ngram_counts = Counter(ngrams(bigtxt.split(), 3)) #split text to tokens\n",
    "ngram_counts.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('Received:', 'from'), 12579),\n",
       " (('with', 'ESMTP'), 7188),\n",
       " (('ESMTP', 'id'), 7188),\n",
       " (('Dec', '2007'), 7063),\n",
       " (('Nov', '2007'), 6810),\n",
       " (('-0500', 'Received:'), 5843),\n",
       " (('for', '<source@collab.sakaiproject.org>;'), 5391),\n",
       " (('text/plain;', 'charset=UTF-8'), 5391),\n",
       " (('+0000', '(GMT)'), 4932),\n",
       " (('from', 'murder'), 3594)]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigtxt = open('mbox.txt').read()\n",
    "ngram_counts = Counter(ngrams(bigtxt.split(), 2))\n",
    "ngram_counts.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOW vector generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectors as features\n",
    "Count Vector (https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) is a matrix notation of the dataset in which every row represents a document from the corpus, every column represents a term from the corpus, and every cell represents the frequency count of a particular term in a particular document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'unc': 9, 'played': 6, 'duke': 2, 'in': 4, 'basketball': 1, 'lost': 5, 'the': 8, 'game': 3, 'ate': 0, 'sandwich': 7}\n",
      "UNC played Duke in basketball = [[0 1 1 0 1 0 1 0 0 1]]\n",
      "Duke lost the basketball game = [[0 1 1 1 0 1 0 0 1 0]]\n",
      "I ate a sandwich = [[1 0 0 0 0 0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "### Create feature vectors using CountVectorizer \n",
    "### binary parameter used to indicate word presence\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\n",
    "    'UNC played Duke in basketball',\n",
    "    'Duke lost the basketball game',\n",
    "    'I ate a sandwich'\n",
    "]\n",
    "vectorizer = CountVectorizer(binary=True) # indicate presence/non-presence\n",
    "# Return a dense matrix representation of this CSR (Compressed Sparse Row) matrix.\n",
    "# https://machinelearningmastery.com/sparse-matrices-for-machine-learning/ \n",
    "X = vectorizer.fit_transform(corpus).todense() \n",
    "\n",
    "print(vectorizer.vocabulary_) # Print the list of words in the vocabulary\n",
    "for i, document in enumerate(corpus):  # Shows condensed version of the feature vectors\n",
    "    print(document, '=', X[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) \n",
      "\n",
      "{'dog': 1, 'ate': 0, 'sandwich': 2, 'wizard': 4, 'transfigured': 3} \n",
      "\n",
      "  (0, 0)\t2\n",
      "  (0, 1)\t1\n",
      "  (0, 2)\t3\n",
      "  (0, 3)\t1\n",
      "  (0, 4)\t1 \n",
      "\n",
      "[[2 1 3 1 1]]\n"
     ]
    }
   ],
   "source": [
    "### CountVectorizer - Creating feature vectors with frequencies of words\n",
    "\n",
    "corpus = ['The dog ate a sandwich, the wizard transfigured a sandwich, \\\n",
    "and I ate a sandwich']\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "### Print the class model with it's parameters\n",
    "print(vectorizer.fit(corpus), '\\n')\n",
    "\n",
    "### Print the list of words in the vocabulary\n",
    "print(vectorizer.vocabulary_, '\\n')\n",
    "\n",
    "### Shows the vector with transformed numerical values (word frequency)\n",
    "print(vectorizer.transform(corpus), '\\n')\n",
    "\n",
    "### Shows condensed version of the feature vectors\n",
    "print(vectorizer.transform(corpus).todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Vectors as features\n",
    "\n",
    "TF-IDF Vectors (https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) can be generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:\n",
      " {'dog': 1, 'ate': 0, 'sandwich': 2, 'wizard': 4, 'transfigured': 3}\n",
      "Count vectors:\n",
      " [[2 1 2 0 0]\n",
      " [0 0 1 1 1]]\n",
      "TF vectors:\n",
      " [[2. 1. 2. 0. 0.]\n",
      " [0. 0. 1. 1. 1.]]\n",
      "TF-IDF vectors:\n",
      " [[2.81093022 1.40546511 2.         0.         0.        ]\n",
      " [0.         0.         1.         1.40546511 1.40546511]]\n"
     ]
    }
   ],
   "source": [
    "### Creating TF and TF-IDF feature vectors using TfidfTransformer. \n",
    "### Note the change in IDF feature values with different number of documents in corpus\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "corpus = [\n",
    "    'The dog ate a sandwich and I ate a sandwich',\n",
    "    'The wizard transfigured a sandwich'\n",
    "]\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "# None for no normalization.\n",
    "transformer = TfidfTransformer(use_idf=False, norm=None) # to get TFIDF values  \n",
    "transformerIDF = TfidfTransformer(use_idf=True, norm=None) # to get IDF values\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print('Vocabulary:\\n', vectorizer.vocabulary_)\n",
    "print('Count vectors:\\n', X.todense())\n",
    "print('TF vectors:\\n', transformer.fit_transform(X).todense())  # divided by the document length\n",
    "print('TF-IDF vectors:\\n', transformerIDF.fit_transform(X).todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.81093022 1.40546511 2.         0.         0.        ]\n",
      " [0.         0.         1.         1.40546511 1.40546511]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "### TfidfVectorizer - combine CountVectorizer and TfidfTransformer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = [\n",
    "    'The dog ate a sandwich and I ate a sandwich',\n",
    "    'The wizard transfigured a sandwich'\n",
    "]\n",
    "vectorizer = TfidfVectorizer(stop_words='english', norm=None)\n",
    "print(vectorizer.fit_transform(corpus).todense(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ate': 1.4054651081081644, 'dog': 1.4054651081081644, 'sandwich': 1.0, 'transfigured': 1.4054651081081644, 'wizard': 1.4054651081081644}\n"
     ]
    }
   ],
   "source": [
    "idf = vectorizer.idf_\n",
    "print (dict(zip(vectorizer.get_feature_names(), idf)))\n",
    "\n",
    "# formula : E.g., \"ate\" in the first column of the first document\n",
    "# df = 1 -- \"ate\" appeared only in one document\n",
    "# N = 2 -- total number of documents\n",
    "# idf = ln((N + 1)/(df + 1)) + 1 = log(3/2) + 1 = 1.4054651081081644\n",
    "\n",
    "# tf = 2\n",
    "# tfidf = tf * idf = 2 * 1.4054651081081644 = 2.81093022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:\n",
      " {'dog': 1, 'ate': 0, 'sandwich': 2, 'wizard': 4, 'transfigured': 3}\n",
      "Count vectors:\n",
      " [[2 1 2 0 0]\n",
      " [0 0 1 1 1]]\n",
      "TF vectors:\n",
      " [[0.66666667 0.33333333 0.66666667 0.         0.        ]\n",
      " [0.         0.         0.57735027 0.57735027 0.57735027]]\n",
      "TF-IDF vectors:\n",
      " [[0.75458397 0.37729199 0.53689271 0.         0.        ]\n",
      " [0.         0.         0.44943642 0.6316672  0.6316672 ]]\n"
     ]
    }
   ],
   "source": [
    "### Norm='l2' used to normalize term vectors.\n",
    "### Creating TF and TF-IDF feature vectors using TfidfTransformer. \n",
    "### Note the change in IDF feature values with different number of documents in corpus\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "corpus = [\n",
    "    'The dog ate a sandwich and I ate a sandwich',\n",
    "    'The wizard transfigured a sandwich'\n",
    "]\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "transformer = TfidfTransformer(use_idf=False)     # Norm='l2' used to normalize term vectors.\n",
    "transformerIDF = TfidfTransformer(use_idf=True)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print('Vocabulary:\\n', vectorizer.vocabulary_)\n",
    "print('Count vectors:\\n', X.todense())\n",
    "print('TF vectors:\\n', transformer.fit_transform(X).todense())  # divided by the document length\n",
    "print('TF-IDF vectors:\\n', transformerIDF.fit_transform(X).todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.75458397 0.37729199 0.53689271 0.         0.        ]\n",
      " [0.         0.         0.44943642 0.6316672  0.6316672 ]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Norm='l2' used to normalize term vectors.\n",
    "### TfidfVectorizer - combine CountVectorizer and TfidfTransformer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = [\n",
    "    'The dog ate a sandwich and I ate a sandwich',\n",
    "    'The wizard transfigured a sandwich'\n",
    "]\n",
    "vectorizer = TfidfVectorizer(stop_words='english')      # Norm='l2' used to normalize term vectors.\n",
    "print(vectorizer.fit_transform(corpus).todense(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
